{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from smogn import smoter\n",
    "import smogn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('annonimized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    \"concat('it001',`assignment_id`)\": 'assignment_id',\n",
    "    \"concat('it001',`problem_id`)\": 'problem_id',\n",
    "    \"concat('it001', username)\": 'username',\n",
    "    \"concat('it001',`language_id`)\": 'language_id'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl = pd.read_csv('th-public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tl.rename(columns={\n",
    "    \"hash\": 'username'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = df_tl[df_tl['username'].isin(df['username'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df['TH'] = pd.to_numeric(df_tl['TH'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing = df.isnull().sum().sum()\n",
    "\n",
    "print(\"Total number of missing values in the DataFrame:\")\n",
    "print(total_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_try(score_df, df):\n",
    "    try_count = df.groupby('username').size().reset_index(name='try')\n",
    "    score_df = pd.merge(score_df, try_count, on='username', how='left')\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = create_try(score_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_solved(score_df, df):\n",
    "    # Select relevant columns\n",
    "    user_final_df = df[['username', 'is_final', 'pre_score']]\n",
    "    \n",
    "    # Filter rows where 'is_final' > 0 and 'pre_score' == 10000\n",
    "    solved_df = user_final_df[(user_final_df['is_final'] > 0) & (user_final_df['pre_score'] == 10000)]\n",
    "    \n",
    "    # Group by 'username' and sum the 'is_final' column\n",
    "    solved_df = solved_df.groupby('username').agg({'is_final': 'sum'})\n",
    "    \n",
    "    # Rename the 'is_final' column to 'solved'\n",
    "    solved_df.rename(columns={'is_final':'solved'}, inplace=True)\n",
    "    \n",
    "    # Reset index to make 'username' a column again\n",
    "    solved_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Merge with the original score_df\n",
    "    score_df = score_df.merge(solved_df, on='username', how='left')\n",
    "    \n",
    "    # Fill NaN values with 0.0\n",
    "    # score_df.fillna(0.0, inplace=True)\n",
    "    \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = create_solved(score_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_num_problem_submit(score_df, df):\n",
    "    filtered_df = df[df['is_final'] == 1]\n",
    "    unique_assignments = filtered_df.groupby('username')['problem_id'].nunique().reset_index(name='num_problems_submited')\n",
    "    score_df = score_df.merge(unique_assignments, on='username', how='left')\n",
    "    score_df['try'] = score_df['try'].fillna(0).astype(int)\n",
    "    return score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = create_num_problem_submit(score_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_num_assignment_submit(score_df, df):\n",
    "    filtered_df = df[df['is_final'] == 1]\n",
    "    unique_assignments = filtered_df.groupby('username')['assignment_id'].nunique().reset_index(name='num_assignments_submited')\n",
    "    score_df = score_df.merge(unique_assignments, on='username', how='left')\n",
    "    score_df['try'] = score_df['try'].fillna(0).astype(int)\n",
    "    return score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = create_num_assignment_submit(score_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def create_total_score(score_df, df):\n",
    "    user_score_df = df[['username', 'is_final', 'pre_score']]\n",
    "    score = user_score_df[user_score_df['is_final'] > 0].groupby('username').sum()\n",
    "    score = score.drop('is_final', axis=1)\n",
    "    score.rename(columns={'pre_score': 'sum_score'}, inplace=True)\n",
    "    score_df = pd.merge(score_df, score, on='username', how='left')\n",
    "    return score_df\n",
    "'''\n",
    "def create_total_score(score_df, df):\n",
    "    # Select relevant columns from df\n",
    "    user_score_df = df[['username', 'is_final', 'pre_score', 'coefficient']]\n",
    "    \n",
    "    # Apply the coefficient to pre_score\n",
    "    user_score_df['adjusted_score'] = user_score_df['pre_score'] / 100 * user_score_df['coefficient']\n",
    "    \n",
    "    # Filter out rows where is_final is less than or equal to 0\n",
    "    filtered_score_df = user_score_df[user_score_df['is_final'] > 0]\n",
    "    \n",
    "    # Group by username and sum the adjusted scores\n",
    "    score = filtered_score_df.groupby('username')['adjusted_score'].sum().reset_index()\n",
    "    \n",
    "    # Rename the column as sum_score\n",
    "    score.rename(columns={'adjusted_score': 'sum_score'}, inplace=True)\n",
    "    \n",
    "    # Merge the summed scores back into the original score_df\n",
    "    score_df = pd.merge(score_df, score, on='username', how='left')\n",
    "    \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = create_total_score(score_df, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_per_problem(score_df):\n",
    "    score_df['score_per_problem'] = score_df['sum_score'] / score_df['num_problems_submited']\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_score_per_problem(score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log_transformation(df, column, shift_value=1):\n",
    "    \"\"\"\n",
    "    Apply a log transformation to a specified column in a DataFrame.\n",
    "    If the column contains zero or negative values, shift the data by shift_value.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    column (str): The name of the column to transform.\n",
    "    shift_value (float): The value to add to the column to shift the data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with the log-transformed column.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Shift the data if there are zero or negative values\n",
    "    if (df[column] <= 0).any():\n",
    "        df_transformed = df.copy()\n",
    "        df_transformed[column] = np.log(df_transformed[column] + shift_value)\n",
    "        print(f\"Data shifted by adding {shift_value} to handle zero or negative values.\")\n",
    "    else:\n",
    "        df_transformed = df.copy()\n",
    "        df_transformed[column] = np.log(df_transformed[column])\n",
    "    \n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##score_df = apply_log_transformation(score_df, 'TH')\n",
    "##score_df = apply_log_transformation(score_df, 'try')\n",
    "##score_df = apply_log_transformation(score_df, 'sum_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = score_df.select_dtypes(include='number').columns\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for col in numeric_cols:\n",
    "    sns.kdeplot(score_df[col], label=col)\n",
    "    plt.title('KDE Plot for All Numeric Columns')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = score_df.groupby('TH')['sum_score'].mean().reset_index()\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(grouped_df['TH'], grouped_df['sum_score'], color='blue', label='Mean of sum_score')\n",
    "plt.xlabel('TH')\n",
    "plt.ylabel('Mean of sum_score')\n",
    "plt.title('TH vs Mean of sum_score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing = score_df.isnull().sum().sum()\n",
    "\n",
    "print(\"Total number of missing values in the DataFrame:\")\n",
    "print(total_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize = (40,30))\n",
    "gs1 = fig.add_gridspec(3,3)\n",
    "gs1.update(wspace=0.4, hspace=0.4)\n",
    "\n",
    "axs0 = fig.add_subplot(gs1[0,0])\n",
    "axs1 = fig.add_subplot(gs1[0,1])\n",
    "axs2 = fig.add_subplot(gs1[1,0])\n",
    "\n",
    "axes=[axs0,axs1,axs2]\n",
    "background_color = '#f6f5f7'\n",
    "\n",
    "for i in axes:\n",
    "    i.set_facecolor(background_color)\n",
    "fig.patch.set_facecolor(background_color) \n",
    "sns.scatterplot(ax = axs0, x = score_df['try'], y = score_df['TH'].sort_values() )\n",
    "axs0.set_title('Solved Assignments and Grade Final')\n",
    "\n",
    "fig.patch.set_facecolor(background_color) \n",
    "sns.scatterplot(ax = axs1, x = score_df['solved'], y = score_df['TH'].sort_values())\n",
    "axs1.set_title('Solved/Tries Assignments and Grade Final')\n",
    "\n",
    "fig.patch.set_facecolor(background_color) \n",
    "sns.scatterplot(ax = axs2,x = score_df['sum_score'], y = score_df['TH'].sort_values())\n",
    "axs2.set_title('Submitted Assignments and Grade Final')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.drop_duplicates(subset=['username', 'assignment_id'])\n",
    "\n",
    "# Step 2: Count the frequency of each assignment_id\n",
    "assignment_counts = grouped_df['assignment_id'].value_counts().reset_index()\n",
    "assignment_counts.columns = ['assignment_id', 'count']\n",
    "\n",
    "# Step 3: Sort the counts in descending order\n",
    "sorted_assignment_counts = assignment_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(\"Sorted assignment counts:\")\n",
    "print(sorted_assignment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = score_df.select_dtypes(include='number').corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = score_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.drop(columns=['username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[~((test_df['sum_score'] > 600000) & (test_df['sum_score'] < 700000) & (test_df['TH'] > 4) & (test_df['TH'] < 6))]\n",
    "test_df = test_df[~((test_df['sum_score'] > 700000) & (test_df['TH'] < 6))]\n",
    "test_df = test_df[~((test_df['sum_score'] < 300000) & (test_df['TH'] > 9))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = score_df.select_dtypes(include='number').columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col == 'TH':\n",
    "        continue\n",
    "    grouped_df = test_df.groupby('TH')[col].mean().reset_index()\n",
    "\n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(grouped_df['TH'], grouped_df[col], color='blue', label='Mean of sum_score')\n",
    "    plt.xlabel('TH')\n",
    "    plt.ylabel(f'Mean of ' + col)\n",
    "    plt.title('TH vs Mean of ' + col)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df.drop(columns=['TH'])\n",
    "y = test_df['TH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##X = oversampled_df.drop(columns=['TH'])\n",
    "##y = oversampled_df['TH']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "\n",
    "X_train = poly.fit_transform(X_train)\n",
    "\n",
    "X_test = poly.transform(X_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'gamma': 0,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eta': 0.01,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Prepare the dataset and train the model\n",
    "train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "test_data = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "evals = [(train_data, 'train'), (test_data, 'eval')]\n",
    "model = xgb.train(params, train_data, num_boost_round=100, evals=evals, early_stopping_rounds=10)\n",
    "\n",
    "y_pred = model.predict(test_data)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "plt.show()\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot Actual vs. Predicted Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Ideal Fit')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Plot Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 2})\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.axhline(0, linestyle='--', color='gray')\n",
    "plt.show()\n",
    "\n",
    "# Assuming df and df_tl are defined somewhere in your code\n",
    "predict_df = df[~df['username'].isin(df_tl['username'])]\n",
    "unique_usernames_df = predict_df[['username']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def create_predict_csv(predict_df, df, model):\n",
    "    \"\"\"\n",
    "    Process predict_df, predict TBTL, and return a DataFrame with MSSV and TBTL.\n",
    "    \n",
    "    Parameters:\n",
    "        predict_df (pd.DataFrame): DataFrame containing usernames to predict.\n",
    "        df (pd.DataFrame): Original DataFrame used for aggregation and calculations.\n",
    "        model: Trained model for predicting TBTL.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing 'MSSV' and predicted 'TBTL'.\n",
    "    \"\"\"\n",
    "    # Step 1: Copy and process raw data\n",
    "    raw = predict_df.copy()\n",
    "    raw = create_try(raw, df)\n",
    "    raw = create_solved(raw, df)\n",
    "    raw = create_num_problem_submit(raw, df)\n",
    "    raw = create_num_assignment_submit(raw, df)\n",
    "    raw = create_total_score(raw, df)\n",
    "    raw = create_score_per_problem(raw)\n",
    "\n",
    "    # Step 2: Fill NaN values in numeric columns with column medians\n",
    "    numeric_cols = raw.select_dtypes(include=['number']).columns\n",
    "    raw[numeric_cols] = raw[numeric_cols].fillna(raw[numeric_cols].median())\n",
    "\n",
    "    # Step 3: Prepare data for prediction\n",
    "    X_predict = raw.drop(columns=['username'])  # Exclude 'username' from features\n",
    "    usernames = raw['username']  # Save usernames for the final output\n",
    "\n",
    "    X_predict = poly.transform(X_predict)\n",
    "\n",
    "    # Step 4: Predict TBTL using the trained model\n",
    "    predict_data = xgb.DMatrix(X_predict)  # Create DMatrix for prediction\n",
    "    TBTL_predictions = model.predict(predict_data)\n",
    "\n",
    "    # Step 5: Create a DataFrame with MSSV and predicted TBTL, rounded to 2 decimal places\n",
    "    results_df = pd.DataFrame({\n",
    "        'username': usernames,\n",
    "        'TBTL': TBTL_predictions\n",
    "    })\n",
    "    results_df['TBTL'] = results_df['TBTL'].round(2)  # Round TBTL to 2 decimal places\n",
    "\n",
    "    return results_df\n",
    "\n",
    "result = create_predict_csv(unique_usernames_df, df, model)\n",
    "\n",
    "result.to_csv('result_tbtl.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly_features', PolynomialFeatures(degree=2)),\n",
    "    ('lasso', LassoCV(alphas=[0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.1, 1, 10], cv=12, random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = pipeline.named_steps['lasso'].alpha_\n",
    "print(f\"Best alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R^2 (Coefficient of Determination): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "# Step 5: Plot Residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 2})\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.axhline(0, linestyle='--', color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = df[~df['username'].isin(df_tl['username'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_usernames_df = predict_df[['username']].drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predict_csv(predict_df, df, model):\n",
    "    \"\"\"\n",
    "    Process predict_df, predict TBTL, and return a DataFrame with MSSV and TBTL.\n",
    "    \n",
    "    Parameters:\n",
    "        predict_df (pd.DataFrame): DataFrame containing usernames to predict.\n",
    "        df (pd.DataFrame): Original DataFrame used for aggregation and calculations.\n",
    "        model: Trained model for predicting TBTL.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing 'MSSV' and predicted 'TBTL'.\n",
    "    \"\"\"\n",
    "    # Step 1: Copy and process raw data\n",
    "    raw = predict_df.copy()\n",
    "    raw = create_try(raw, df)\n",
    "    raw = create_solved(raw, df)\n",
    "    raw = create_num_problem_submit(raw, df)\n",
    "    raw = create_total_score(raw, df)\n",
    "    ##raw = create_try_per_solve(raw)\n",
    "    ##raw = create_submit_per_solve(raw)\n",
    "    ##raw = apply_log_transformation(raw, 'try')\n",
    "    ##raw = apply_log_transformation(raw, 'sum_score')\n",
    "\n",
    "    ##raw = calculate_solved_per_try(raw)\n",
    "\n",
    "    # Step 2: Fill NaN values in numeric columns with column medians\n",
    "    numeric_cols = raw.select_dtypes(include=['number']).columns\n",
    "    raw[numeric_cols] = raw[numeric_cols].fillna(raw[numeric_cols].median())\n",
    "\n",
    "    # Step 3: Prepare data for prediction\n",
    "    X_predict = raw.drop(columns=['username'])  # Exclude 'username' from features\n",
    "    usernames = raw['username']  # Save usernames for the final output\n",
    "\n",
    "    # Step 4: Predict TBTL using the trained model\n",
    "    TBTL_predictions = model.predict(X_predict)\n",
    "\n",
    "    # Step 5: Create a DataFrame with MSSV and predicted TBTL, rounded to 2 decimal places\n",
    "    results_df = pd.DataFrame({\n",
    "        'username': usernames,\n",
    "        'TBTL': TBTL_predictions\n",
    "    })\n",
    "    results_df['TBTL'] = results_df['TBTL'].round(2)  # Round TBTL to 2 decimal places\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = create_predict_csv(unique_usernames_df, df, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_tbtl.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
